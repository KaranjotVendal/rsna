{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KaranjotVendal\\mambaforge\\envs\\thesis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\KaranjotVendal\\mambaforge\\envs\\thesis\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\KaranjotVendal\\mambaforge\\envs\\thesis\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "from datamodule import RSNAdataset\n",
    "from config import config\n",
    "from gradcam import GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvxLSTM(nn.Module):\n",
    "    '''ConvNext: pretrained IMAGENET, not trainable\n",
    "        LSTM: 1 layer, 64 units, unidirectional\n",
    "        FC: 32 units\n",
    "        classifier: 2 units'''\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = timm.create_model('convnextv2_tiny.fcmae_ft_in22k_in1k', pretrained=True, num_classes=0, in_chans=1)\n",
    "        if config.USE_ft_convnext:\n",
    "            checkpoint = torch.load(f'./data/pretrain_convnext/ConvNext_finetuned_model_best_auroc.pth')\n",
    "            self.cnn.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = True\n",
    "        in_features = self.cnn(torch.randn(2, 1, config.IMG_SIZE, config.IMG_SIZE)).shape[1]\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=in_features, hidden_size=config.RNN, batch_first= True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(config.N_SLICES * config.RNN, config.FC, bias=True)\n",
    "        self.classifier = nn.Linear(config.FC, num_classes, bias=True)        \n",
    "        \n",
    "    def forward(self, x, org):\n",
    "        # x shape: BxSxCxHxW\n",
    "        batch_size, slices, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * slices, C, H, W)\n",
    "    \n",
    "        out = self.cnn(c_in)\n",
    "        rnn_in = out.view(batch_size, slices, -1)\n",
    "        out, hd = self.rnn(rnn_in)\n",
    "        mask = self.mask_layer(org)\n",
    "        out = out * mask\n",
    "        batch, slices, rnn_features = out.size()\n",
    "        out = out.reshape(batch, slices * rnn_features)\n",
    "        out = F.relu(self.fc(out))\n",
    "        logits = self.classifier(out)\n",
    "        output = F.softmax(logits, dim=1)\n",
    "        \n",
    "        return logits, output\n",
    "\n",
    "    def mask_layer(self, org):\n",
    "        masks = []\n",
    "        org = org[0].cpu().numpy()\n",
    "        for i in org:\n",
    "            dup = config.N_SLICES - i\n",
    "            mask_1 = torch.ones(i, config.RNN) # .to(device='cuda')\n",
    "            mask_0 = torch.zeros(dup, config.RNN) #.to(device='cuda')\n",
    "            mask = torch.cat((mask_1, mask_0), 0)\n",
    "            masks.append(mask)\n",
    "\n",
    "        masks = torch.stack(masks).to(config.DEVICE)\n",
    "        return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvxLSTM(\n",
       "  (cnn): ConvNeXt(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (stages): Sequential(\n",
       "      (0): ConvNeXtStage(\n",
       "        (downsample): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ConvNeXtStage(\n",
       "        (downsample): Sequential(\n",
       "          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ConvNeXtStage(\n",
       "        (downsample): Sequential(\n",
       "          (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (3): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (4): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (5): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (6): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (7): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (8): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ConvNeXtStage(\n",
       "        (downsample): Sequential(\n",
       "          (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNeXtBlock(\n",
       "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): GlobalResponseNormMlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (grn): GlobalResponseNorm()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_pre): Identity()\n",
       "    (head): NormMlpClassifierHead(\n",
       "      (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
       "      (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (pre_logits): Identity()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (fc): Identity()\n",
       "    )\n",
       "  )\n",
       "  (rnn): LSTM(768, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=16000, out_features=32, bias=True)\n",
       "  (classifier): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvxLSTM(config.NUM_CLASSES)\n",
    "checkpoint = torch.load(f'.\\experiments2.0\\experiment 3\\weights\\ConvxLSTM_model_FLAIR_3.pth')\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_layer = model.cnn.stages[3].blocks[2].conv_dw\n",
    "target_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt = []\n",
    "empty_fld = [109, 123, 709]\n",
    "df = pd.read_csv(\"data/train_labels.csv\")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "X = df['BraTS21ID'].values\n",
    "Y = df['MGMT_value'].values\n",
    "\n",
    "for i in empty_fld:\n",
    "    j = np.where(X == i)\n",
    "    dlt.append(j)\n",
    "    X = np.delete(X, j)\n",
    "    \n",
    "Y = np.delete(Y,dlt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_idx, test_idx) in enumerate(skf.split(np.zeros(len(Y)), Y), 1):  \n",
    "\n",
    "    xtrain = X[train_idx]\n",
    "    ytrain = Y[train_idx]\n",
    "    xtest = X[test_idx]\n",
    "    ytest = Y[test_idx]\n",
    "\n",
    "    train_set = RSNAdataset(\n",
    "                    config.DATA_PATH,\n",
    "                    xtrain,  \n",
    "                    ytrain,\n",
    "                    n_slices=config.N_SLICES,\n",
    "                    img_size=config.IMG_SIZE,\n",
    "                    type = config.MOD,\n",
    "                    transform=None\n",
    "                        )\n",
    "\n",
    "    test_set = RSNAdataset(\n",
    "                    config.DATA_PATH,\n",
    "                    xtest,  \n",
    "                    ytest,\n",
    "                    n_slices=config.N_SLICES,\n",
    "                    img_size=config.IMG_SIZE,\n",
    "                    type = config.MOD,\n",
    "                    transform=None\n",
    "                        )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "                train_set,    \n",
    "                batch_size=1,\n",
    "                shuffle=True,\n",
    "                num_workers=config.NUM_WORKERS,\n",
    "            )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "                test_set,    \n",
    "                batch_size=1,\n",
    "                shuffle=False,\n",
    "                num_workers=config.NUM_WORKERS,\n",
    "            )\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = RSNAdataset(\n",
    "                config.DATA_PATH,\n",
    "                X,  \n",
    "                Y,\n",
    "                n_slices=config.N_SLICES,\n",
    "                img_size=config.IMG_SIZE,\n",
    "                type = config.MOD,\n",
    "                transform=None\n",
    "                )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "                train_set,    \n",
    "                batch_size=1,\n",
    "                shuffle=False,\n",
    "                num_workers=config.NUM_WORKERS,\n",
    "            )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([1, 250, 1, 112, 112])\n",
      "Image Class dimensions: torch.Size([1])\n",
      "Image original slices [tensor([35])]\n",
      "Image batch dimensions: torch.Size([1, 250, 1, 112, 112])\n",
      "Image Class dimensions: torch.Size([1])\n",
      "Image original slices [tensor([202])]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:  \n",
    "    print('Image batch dimensions:', batch['X'].shape)\n",
    "    print('Image Class dimensions:', batch['y'].shape)\n",
    "    print('Image original slices', batch['org'])\n",
    "    train_image = batch['X']\n",
    "    train_org = batch['org']\n",
    "    break\n",
    "\n",
    "for batch in test_loader:  \n",
    "    print('Image batch dimensions:', batch['X'].shape)\n",
    "    print('Image Class dimensions:', batch['y'].shape)\n",
    "    print('Image original slices', batch['org'])\n",
    "    test_image = batch['X']\n",
    "    test_org = batch['org']\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = GradCAM(model, target_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward hook running...\n",
      "Activations size: torch.Size([250, 768, 3, 3])\n",
      "prediction: tensor([0], device='cuda:0')\n",
      "Backward hook running...\n",
      "Gradients size: torch.Size([250, 768, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "heatmap, images = cam.compute_cam(train_image, train_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(heatmap[1, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resized_heatmap(heatmap, shape):\n",
    "    \"\"\"Resize heatmap to shape: batch_size, slices, C, H, W\"\"\"\n",
    "    # Given the provided shape, we're interested in resizing based on the slices, H, and W dimensions.\n",
    "    \n",
    "    # Rescale heatmap to a range 0-255\n",
    "    upscaled_heatmap = np.uint8(255 * heatmap)\n",
    "    print(upscaled_heatmap.shape)\n",
    "    print(shape)\n",
    "\n",
    "    # Calculate zoom factors for each dimension\n",
    "    upscaled_heatmap = zoom(\n",
    "        upscaled_heatmap,\n",
    "        (\n",
    "            shape[0] / upscaled_heatmap.shape[0],  # slices dimension\n",
    "            shape[2] / upscaled_heatmap.shape[1],  # H dimension\n",
    "            shape[3] / upscaled_heatmap.shape[2],  # W dimension\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return upscaled_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 3, 3)\n",
      "(250, 1, 112, 112)\n"
     ]
    }
   ],
   "source": [
    "resized_heatmap = get_resized_heatmap(heatmap, images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_heatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(2, 2))\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "ax[0].get_yaxis().set_visible(False)\n",
    "\n",
    "ax[1].get_xaxis().set_visible(False)\n",
    "ax[1].get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "# Assuming batch size is 1 for simplicity. If batch size > 1, you can change the index to visualize other samples.\n",
    "slice_idx = 60  # The slice number you want to visualize\n",
    "image = images[slice_idx]\n",
    "image = image.squeeze(0)\n",
    "\n",
    "# Display the original slice on the left\n",
    "ax[0].imshow(image, cmap='bone')\n",
    "\n",
    "# Display the original slice on the right\n",
    "img0 = ax[1].imshow(image, cmap='bone')\n",
    "\n",
    "# Overlay the heatmap on the right image\n",
    "img1 = ax[1].imshow(resized_heatmap[slice_idx, :, :], cmap='jet', alpha=0.3, extent=img0.get_extent())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./gradcam_img{1}.png', dpi=300, bbox_inches='tight', format='png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./plots'):\n",
    "    os.mkdir(base_dir)\n",
    "if not os.path.exists('./plots/GradCAM/'):\n",
    "    os.mkdir('./plots/GradCAM')\n",
    "if not os.path.exists('./plots/GradCAM/train/'):\n",
    "    os.mkdir('./plots/GradCAM/train')    \n",
    "\n",
    "for i in range(train_org[0]):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(2, 2))\n",
    "    ax[0].get_xaxis().set_visible(False)\n",
    "    ax[0].get_yaxis().set_visible(False)\n",
    "\n",
    "    ax[1].get_xaxis().set_visible(False)\n",
    "    ax[1].get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "    # Assuming batch size is 1 for simplicity. If batch size > 1, you can change the index to visualize other samples.\n",
    "    slice_idx = i  # The slice number you want to visualize\n",
    "    image = images[slice_idx]\n",
    "    image = image.squeeze(0)\n",
    "\n",
    "    # Display the original slice on the left\n",
    "    ax[0].imshow(image, cmap='bone')\n",
    "\n",
    "    # Display the original slice on the right\n",
    "    img0 = ax[1].imshow(image, cmap='bone')\n",
    "\n",
    "    # Overlay the heatmap on the right image\n",
    "    img1 = ax[1].imshow(resized_heatmap[slice_idx, :, :], cmap='jet', alpha=0.4, extent=img0.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./plots/GradCAM/train/gradcam_train_img{i}.png', dpi=300, bbox_inches='tight', format='png')\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = GradCAM(model, target_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward hook running...\n",
      "Activations size: torch.Size([250, 768, 3, 3])\n",
      "prediction: tensor([1], device='cuda:0')\n",
      "Backward hook running...\n",
      "Gradients size: torch.Size([250, 768, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "heatmap, images = cam.compute_cam(test_image, test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(heatmap[2, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 3, 3)\n",
      "(250, 1, 112, 112)\n"
     ]
    }
   ],
   "source": [
    "resized_heatmap = get_resized_heatmap(heatmap, images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./plots'):\n",
    "    os.mkdir(base_dir)\n",
    "if not os.path.exists('./plots/GradCAM/'):\n",
    "    os.mkdir('./plots/GradCAM')\n",
    "if not os.path.exists('./plots/GradCAM/test/'):\n",
    "    os.mkdir('./plots/GradCAM/test/')\n",
    "\n",
    "for i in range(test_org[0]):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(2, 2))\n",
    "    ax[0].get_xaxis().set_visible(False)\n",
    "    ax[0].get_yaxis().set_visible(False)\n",
    "\n",
    "    ax[1].get_xaxis().set_visible(False)\n",
    "    ax[1].get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "    # Assuming batch size is 1 for simplicity. If batch size > 1, you can change the index to visualize other samples.\n",
    "    slice_idx = i  # The slice number you want to visualize\n",
    "    image = images[slice_idx]\n",
    "    image = image.squeeze(0)\n",
    "\n",
    "    # Display the original slice on the left\n",
    "    ax[0].imshow(image, cmap='bone')\n",
    "\n",
    "    # Display the original slice on the right\n",
    "    img0 = ax[1].imshow(image, cmap='bone')\n",
    "\n",
    "    # Overlay the heatmap on the right image\n",
    "    img1 = ax[1].imshow(resized_heatmap[slice_idx, :, :], cmap='jet', alpha=0.4, extent=img0.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./plots/GradCAM/test/gradcam_test_img{i}.png', dpi=300, bbox_inches='tight', format='png')\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
