{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'! pip install lightning\\n! pip install torchmetrics\\n! pip install watermark\\n! pip install mlxtend\\n! pip install tensorboard\\n! pip install pandas'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''! pip install lightning\n",
    "! pip install torchmetrics\n",
    "! pip install watermark\n",
    "! pip install mlxtend\n",
    "! pip install pandas'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torchmetrics\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datamodules import RSNAdataset\n",
    "from utils import LossMeter\n",
    "from plotting import show_failures, plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaranjot\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/karanjotvendal/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='a2a7828ed68b3cba08f2703971162138c680b664')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Karanjot Vendal\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.4\n",
      "IPython version      : 8.14.0\n",
      "\n",
      "torch: 2.0.1\n",
      "\n",
      "torchvision : 0.15.2\n",
      "numpy       : 1.25.1\n",
      "torch       : 2.0.1\n",
      "timm        : 0.9.2\n",
      "matplotlib  : 3.7.2\n",
      "wandb       : 0.15.7\n",
      "torchmetrics: 1.0.1\n",
      "pandas      : 2.0.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a 'Karanjot Vendal' -v -p torch --iversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/reduced_dataset/'\n",
    "MODEL = 'resnet18'\n",
    "BATCH_SIZE = 3\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "KFOLD= 10 \n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"RACNet\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"experiment_{1}\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": LEARNING_RATE,\n",
    "      \"architecture\": \"CNN-GRU-MASK-FC-Classifier\",\n",
    "      \"dataset\": \"MICAA MRI\",\n",
    "      \"epochs\": NUM_EPOCHS,\n",
    "      \"BATCH_SIZE\": BATCH_SIZE\n",
    "      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RACNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = timm.create_model(MODEL, pretrained=True, num_classes=0, in_chans=1)\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        in_features = self.cnn(torch.randn(2, 1, 112, 112)).shape[1]\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=in_features, hidden_size=64, batch_first= True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(16256, 32, bias=True)\n",
    "        self.classifier = nn.Linear(32, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x, org):\n",
    "        # x shape: BxSxCxHxW\n",
    "        batch_size, slices, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * slices, C, H, W)\n",
    "        print('reshape input', c_in.shape)\n",
    "        \n",
    "        out = self.cnn(c_in)\n",
    "        print('CNN ouput', out.shape)\n",
    "        \n",
    "        rnn_in = out.view(batch_size, slices, -1)\n",
    "        print('reshaped rnn_in', rnn_in.shape)\n",
    "        out, hd = self.rnn(rnn_in)\n",
    "        #out =F.relu(self.RNN(out))\n",
    "\n",
    "        print('RNN ouput', out.shape)\n",
    "        mask = self.mask_layer(org)\n",
    "        out = out * mask\n",
    "        print('mask ouput', out.shape)\n",
    "        \n",
    "        batch, slices, rnn_features = out.size()\n",
    "        out = out.reshape(batch_size, slices * rnn_features)\n",
    "        print('reshaped masked output', out.shape)\n",
    "        \n",
    "        out = F.relu(self.fc(out))\n",
    "        print('fc ouput', out.shape)\n",
    "        \n",
    "        logits = self.classifier(out)\n",
    "        print('logits', logits.shape)\n",
    "        \n",
    "        output = F.softmax(logits, dim=1)\n",
    "        \n",
    "        print('classifier ouput', logits.shape)\n",
    "        #[prob 0, prob 1]\n",
    "\n",
    "        return logits, output\n",
    "\n",
    "    def mask_layer(self, org):\n",
    "        masks = []\n",
    "        org = org[0].cpu().numpy()\n",
    "        for i in org:\n",
    "            #print(i)\n",
    "            dup = 254 - i\n",
    "            mask_1 = torch.ones(i, 64) # .to(device='cuda')\n",
    "            mask_0 = torch.zeros(dup, 64) #.to(device='cuda')\n",
    "            mask = torch.cat((mask_1, mask_0), 0)\n",
    "            masks.append(mask)\n",
    "\n",
    "        masks = torch.stack(masks).to(device='cuda')\n",
    "        return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdm\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_dataloader():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#train_counter.update(labels.tolist())\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#print('X',batch['X'])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#print('y',batch['y'])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morg\u001b[39m\u001b[38;5;124m'\u001b[39m,batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morg\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#for i in batch['org']:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#    print(i.items())\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dm' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in dm.train_dataloader():\n",
    "    #train_counter.update(labels.tolist())\n",
    "    #print('X',batch['X'])\n",
    "    #print('y',batch['y'])\n",
    "    print('org',batch['org'][0][1])\n",
    "    #for i in batch['org']:\n",
    "    #    print(i.items())\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion,\n",
    "        epochs,\n",
    "        loss_meter, \n",
    "        fold\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.loss_meter = loss_meter\n",
    "        self.hist = {'test_acc':[],\n",
    "                     'test_f1':[],\n",
    "                     'test_roc':[],\n",
    "                     'train_loss':[],\n",
    "                     'train_acc':[],\n",
    "                     'train_f1': [],\n",
    "                     'train_roc': [],\n",
    "                    }\n",
    "        \n",
    "        self.best_valid_score = -np.inf\n",
    "        self.best_valid_loss = np.inf\n",
    "        self.best_f_score = 0\n",
    "        self.n_patience = 0\n",
    "        self.fold = fold\n",
    "\n",
    "        self.record = {'test_loss':[],\n",
    "                    'test_acc':[],\n",
    "                     'test_f1':[],\n",
    "                     'test_roc':[],\n",
    "                     'train_loss':[],\n",
    "                     'train_acc':[],\n",
    "                     'train_f1': [],\n",
    "                     'train_roc': [],\n",
    "                    }\n",
    "        \n",
    "        \n",
    "    def fit(self, epochs, train_loader, save_path, patience):\n",
    "        train_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            t = time.time()\n",
    "            self.model.train()\n",
    "            train_loss = self.loss_meter()\n",
    "            train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(self.device)\n",
    "            train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=2, average='macro').to(self.device)\n",
    "            train_auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=2).to(self.device)\n",
    "            \n",
    "            for idx, batch in enumerate(tqdm(train_loader)):\n",
    "                print('-'*50)\n",
    "                features = batch['X'].to(self.device)\n",
    "                targets = batch['y'].type(torch.cuda.LongTensor).to(self.device)\n",
    "                org = batch['org']\n",
    "                print(org)\n",
    "                print(\"feature\", features.shape)\n",
    "                ### FORWARD AND BACK PROP\n",
    "                logits, probs = self.model(features, org)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                \n",
    "                train_loss.update(loss.detach().item())\n",
    "                train_acc.update(probs, targets)\n",
    "                train_f1.update(probs, targets)\n",
    "                train_auroc.update(probs, targets)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()  \n",
    "                print(f'-----------------Loss: {loss}-----------------------')\n",
    "                print('------BATCH ENDING-------')\n",
    "            \n",
    "                #print(f'Epoch: {epoch+1}/{epochs} | Loss: {loss:.5f} | Accuracy: {train_acc:.4f}% | F1 Score: {train_f1:.4f} | AUROC: {train_roc:.4f} | Time: {int(time.time() - t)}')\n",
    "            \n",
    "            _loss = train_loss.avg\n",
    "            _acc = train_acc.compute()\n",
    "            _f1 = train_f1.compute()\n",
    "            _roc = train_auroc.compute()\n",
    "            \n",
    "            self.hist['train_loss'].append(_loss)\n",
    "            self.hist['train_acc'].append(_acc)\n",
    "            self.hist['train_f1'].append(_f1)\n",
    "            self.hist['train_auroc'].append(_roc)\n",
    "            \n",
    "            print(f'Epoch: {epoch+1}/{epochs} | Loss: {_loss:.5f} | Accuracy: {_acc:.4f}% | F1 Score: {_f1:.4f} | AUROC: {_roc:.4f} | Time: {time.time() - t}')\n",
    "            \n",
    "            \n",
    "        avg_loss = np.mean(self.hist['train_loss'])\n",
    "        avg_acc = np.mean(self.hist['train_acc'])\n",
    "        avg_f1 = np.mean(self.hist['train_f1'])\n",
    "        avg_auroc = np.mean(self.hist['train_auroc'])\n",
    "\n",
    "        print(f'Epoch Training Time: {(train.time() - start_time)/60} min | Avg Loss: {avg_loss:.5f} | Avg Accuracy: {avg_acc:.4f}% | Avg F1 Score: {avg_f1:.4f} | Avg AUROC: {avg_auroc:.4f}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        #return avg_loss, avg_acc, avg_f1, avg_auroc\n",
    "        \n",
    "        \n",
    "        #testing------------------\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "        test_time = time.time()\n",
    "        test_loss = self.loss_meter()\n",
    "        test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        test_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=2, average='macro')        \n",
    "        test_auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=2)\n",
    "        self.model.eval()\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            with torch.no_grad():    \n",
    "                features = batch['X'].to(self.device)\n",
    "                #targets = batch['y'].type(torch.cuda.LongTensor).to(self.device)\n",
    "                targets = batch['y'].type(torch.cuda.LongTensor).to(self.device)\n",
    "                \n",
    "                org = batch['org']\n",
    "                print(org)\n",
    "                \n",
    "                logits, probas = self.model(features, org)\n",
    "                loss = self.criterion(logits, targets)\n",
    "            \n",
    "                test_loss.update(loss.detach().item())\n",
    "                test_acc.update(probs, targets)\n",
    "                test_f1.update(probs, targets)\n",
    "                test_auroc.update(probs, targets)\n",
    "                print('------BATCH ENDING-------')\n",
    "\n",
    "        _loss = train_loss.avg\n",
    "        _acc = train_acc.compute()\n",
    "        _f1 = train_f1.compute()\n",
    "        _roc = train_auroc.compute()  \n",
    "\n",
    "        self.hist['test_loss'].append(_loss)\n",
    "        self.hist['test_acc'].append(_acc)\n",
    "        self.hist['test_f1'].append(_f1)\n",
    "        self.hist['test_auroc'].append(_roc)\n",
    "            \n",
    "\n",
    "                #print(f'Total Training Time: {(train.time() - start_time)/60} min | Avg Loss: {avg_loss:.5f} | Avg Accuracy: {avg_acc:.4f}% | Avg F1 Score: {avg_f1:.4f} | Avg AUROC: {avg_auroc:.4f}')\n",
    "\n",
    "\n",
    "        avg_loss = np.mean(self.hist['test_loss'])\n",
    "        avg_acc = np.mean(self.hist['test_acc'])\n",
    "        avg_f1 = np.mean(self.hist['test_f1'])\n",
    "        avg_auroc = np.mean(self.hist['test_auroc'])\n",
    "\n",
    "        print(f'Testing Time: {(time.time() - test_time)/60} min | Avg Loss: {avg_loss:.5f} | Avg Accuracy: {avg_acc:.4f}% | Avg F1 Score: {avg_f1:.4f} | Avg AUROC: {avg_auroc:.4f}')\n",
    "        \n",
    "        return avg_loss, avg_acc, avg_f1, avg_auroc\n",
    "                        \n",
    "                \n",
    "    def save_model(self, n_epoch, save_path):\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": self.model.state_dict(),\n",
    "                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                    \"best_valid_score\": self.best_valid_score,\n",
    "                    \"best_f1_score\": self.best_f_score,\n",
    "                    \"n_epoch\": n_epoch,\n",
    "                },\n",
    "                save_path,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path, epochs, n_fold, batch_size, num_workers, device):\n",
    "    \n",
    "    fold_acc = []\n",
    "    fold_loss = []\n",
    "    fold_auroc = []\n",
    "    fold_f1 = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = RACNet(NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = F.cross_entropy\n",
    "        \n",
    "    for _ in range(n_fold):\n",
    "        fold = _+1\n",
    "        folds_xtrain = np.load('./data/folds/xtrain.npy', allow_pickle=True)\n",
    "        folds_xtest = np.load('./data/folds/xtest.npy', allow_pickle=True)\n",
    "        folds_ytrain = np.load('./data/folds/ytrain.npy', allow_pickle=True)\n",
    "        folds_ytest = np.load('./data/folds/ytest.npy', allow_pickle=True)\n",
    "        \n",
    "        xtrain = folds_xtrain[_]\n",
    "        ytrain = folds_ytrain[_]\n",
    "        xtest = folds_xtest[_]\n",
    "        ytest = folds_ytest[_]\n",
    "        \n",
    "        print('-'*30)\n",
    "        print(f\"Fold {fold}\")\n",
    "\n",
    "        dlt = []\n",
    "        for idx, value in enumerate(xtrain):\n",
    "            if value == 109:\n",
    "                dlt.append(idx)\n",
    "            if value ==123:\n",
    "                dlt.append(idx)\n",
    "            if value == 709:\n",
    "                dlt.append(idx)\n",
    "                \n",
    "        for i in dlt:\n",
    "            np.delete(xtrain, i)\n",
    "            np.delete(ytrain, i)\n",
    "\n",
    "        d = []\n",
    "        for idx, value in enumerate(xtest):\n",
    "            if value == 109 or value ==123 or value == 709:\n",
    "                d.append(idx)\n",
    "        for i in d:\n",
    "            np.delete(xtest, i)\n",
    "            np.delete(ytest, i)\n",
    "                \n",
    "\n",
    "        train_set = RSNAdataset(\n",
    "                        './data/reduced_dataset/',\n",
    "                        xtrain,  \n",
    "                        ytrain,\n",
    "                        n_slices=254,\n",
    "                        img_size=112,\n",
    "                        transform=None\n",
    "                            )\n",
    "    \n",
    "        test_set = RSNAdataset(\n",
    "                        './data/reduced_dataset/',\n",
    "                        xtest,  \n",
    "                        ytest,\n",
    "                        n_slices=254,\n",
    "                        img_size=112,\n",
    "                        transform=None\n",
    "                            )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "                    train_set,    \n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    num_workers=num_workers,\n",
    "                )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "                    test_set,    \n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    num_workers=num_workers,\n",
    "                )\n",
    "        #for batch in train_loader:\n",
    "        #    print('X',batch['X'])\n",
    "        #    print('y',batch['y'])\n",
    "        #    print('org',batch['org'])\n",
    "            #for i in batch['org']:\n",
    "            #    print(i.items())\n",
    "         #   break\n",
    "                    \n",
    "        #model = RACNet(NUM_CLASSES)\n",
    "        #model = model.to(DEVICE)\n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        #criterion = F.cross_entropy\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model, \n",
    "            device, \n",
    "            optimizer, \n",
    "            criterion,\n",
    "            epochs,\n",
    "            LossMeter, \n",
    "            fold\n",
    "        )\n",
    "        \n",
    "        trainer.fit(epochs,\n",
    "                    train_loader,\n",
    "                    './checkpoints/f\"best-model-{fold}.pth',\n",
    "                    5)\n",
    "                        \n",
    "        #trainer.plot_loss()\n",
    "        #trainer.plot_score()\n",
    "        #trainer.plot_fscore()\n",
    "                \n",
    "        #test\n",
    "        loss, test_acc, test_f1, test_auroc = Trainer.test(test_loader)\n",
    "        fold_loss.append(loss)\n",
    "        fold_acc.append(test_acc)\n",
    "        fold_f1.append(test_f1)\n",
    "        fold_auroc.append(test_auroc)    \n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    '''wandb.log({\n",
    "         'Avg Test f1 score': np.mean(test_fscore),\n",
    "         'Avg Train f1 score': np.mean(f_scores)\n",
    "         })'''\n",
    "    print('\\nTraining complete in {:.0f}m {:.0f}s'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print('Avg loss {:.5f}'.format(np.mean(losses)))\n",
    "    print('Avg score {:.5f}'.format(np.mean(scores)))\n",
    "    print('Avg Train f1_score {:.5f}'.format(np.mean(f_scores)))\n",
    "    print('Avg Test f1_score {:.5f}'.format(np.mean(test_fscore)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "[tensor([ 96, 101,  31])]\n",
      "feature torch.Size([3, 254, 1, 112, 112])\n",
      "reshape input torch.Size([762, 1, 112, 112])\n",
      "CNN ouput torch.Size([762, 512])\n",
      "reshaped rnn_in torch.Size([3, 254, 512])\n",
      "RNN ouput torch.Size([3, 254, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                                  | 2/156 [00:01<01:13,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask ouput torch.Size([3, 254, 64])\n",
      "reshaped masked output torch.Size([3, 16256])\n",
      "fc ouput torch.Size([3, 32])\n",
      "logits torch.Size([3, 2])\n",
      "classifier ouput torch.Size([3, 2])\n",
      "-----------------Loss: 0.7153329253196716-----------------------\n",
      "------BATCH ENDING-------\n",
      "--------------------------------------------------\n",
      "[tensor([ 93,  34, 102])]\n",
      "feature torch.Size([3, 254, 1, 112, 112])\n",
      "reshape input torch.Size([762, 1, 112, 112])\n",
      "CNN ouput torch.Size([762, 512])\n",
      "reshaped rnn_in torch.Size([3, 254, 512])\n",
      "RNN ouput torch.Size([3, 254, 64])\n",
      "mask ouput torch.Size([3, 254, 64])\n",
      "reshaped masked output torch.Size([3, 16256])\n",
      "fc ouput torch.Size([3, 32])\n",
      "logits torch.Size([3, 2])\n",
      "classifier ouput torch.Size([3, 2])\n",
      "-----------------Loss: 0.7232459187507629-----------------------\n",
      "------BATCH ENDING-------\n",
      "--------------------------------------------------\n",
      "[tensor([111,  20,  46])]\n",
      "feature torch.Size([3, 254, 1, 112, 112])\n",
      "reshape input torch.Size([762, 1, 112, 112])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                                 | 3/156 [00:01<00:51,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN ouput torch.Size([762, 512])\n",
      "reshaped rnn_in torch.Size([3, 254, 512])\n",
      "RNN ouput torch.Size([3, 254, 64])\n",
      "mask ouput torch.Size([3, 254, 64])\n",
      "reshaped masked output torch.Size([3, 16256])\n",
      "fc ouput torch.Size([3, 32])\n",
      "logits torch.Size([3, 2])\n",
      "classifier ouput torch.Size([3, 2])\n",
      "-----------------Loss: 0.7260066866874695-----------------------\n",
      "------BATCH ENDING-------\n",
      "--------------------------------------------------\n",
      "[tensor([ 30, 123,  35])]\n",
      "feature torch.Size([3, 254, 1, 112, 112])\n",
      "reshape input torch.Size([762, 1, 112, 112])\n",
      "CNN ouput torch.Size([762, 512])\n",
      "reshaped rnn_in torch.Size([3, 254, 512])\n",
      "RNN ouput torch.Size([3, 254, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                                | 5/156 [00:01<00:34,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask ouput torch.Size([3, 254, 64])\n",
      "reshaped masked output torch.Size([3, 16256])\n",
      "fc ouput torch.Size([3, 32])\n",
      "logits torch.Size([3, 2])\n",
      "classifier ouput torch.Size([3, 2])\n",
      "-----------------Loss: 0.6910204291343689-----------------------\n",
      "------BATCH ENDING-------\n",
      "--------------------------------------------------\n",
      "[tensor([19, 92, 49])]\n",
      "feature torch.Size([3, 254, 1, 112, 112])\n",
      "reshape input torch.Size([762, 1, 112, 112])\n",
      "CNN ouput torch.Size([762, 512])\n",
      "reshaped rnn_in torch.Size([3, 254, 512])\n",
      "RNN ouput torch.Size([3, 254, 64])\n",
      "mask ouput torch.Size([3, 254, 64])\n",
      "reshaped masked output torch.Size([3, 16256])\n",
      "fc ouput torch.Size([3, 32])\n",
      "logits torch.Size([3, 2])\n",
      "classifier ouput torch.Size([3, 2])\n",
      "-----------------Loss: 0.7030413746833801-----------------------\n",
      "------BATCH ENDING-------\n",
      "--------------------------------------------------\n",
      "[tensor([ 97, 112,  32])]\n",
      "feature torch.Size([3, 254, 1, 112, 112])\n",
      "reshape input torch.Size([762, 1, 112, 112])\n",
      "CNN ouput torch.Size([762, 512])\n",
      "reshaped rnn_in torch.Size([3, 254, 512])\n",
      "RNN ouput torch.Size([3, 254, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                               | 6/156 [00:01<00:44,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask ouput torch.Size([3, 254, 64])\n",
      "reshaped masked output torch.Size([3, 16256])\n",
      "fc ouput torch.Size([3, 32])\n",
      "logits torch.Size([3, 2])\n",
      "classifier ouput torch.Size([3, 2])\n",
      "-----------------Loss: 0.6867237091064453-----------------------\n",
      "------BATCH ENDING-------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 127, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 127, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [254, 1, 1, 112, 112] at entry 0 and [254, 1, 112, 112] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKFOLD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_WORKERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 105\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(path, epochs, n_fold, batch_size, num_workers, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#for batch in train_loader:\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#    print('X',batch['X'])\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#    print('y',batch['y'])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m#criterion = F.cross_entropy\u001b[39;00m\n\u001b[1;32m     95\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     96\u001b[0m     model, \n\u001b[1;32m     97\u001b[0m     device, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     fold\n\u001b[1;32m    103\u001b[0m )\n\u001b[0;32m--> 105\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./checkpoints/f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest-model-\u001b[39;49m\u001b[38;5;132;43;01m{fold}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#trainer.plot_loss()\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#trainer.plot_score()\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m#trainer.plot_fscore()\u001b[39;00m\n\u001b[1;32m    113\u001b[0m         \n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#test\u001b[39;00m\n\u001b[1;32m    115\u001b[0m loss, test_acc, test_f1, test_auroc \u001b[38;5;241m=\u001b[39m Trainer\u001b[38;5;241m.\u001b[39mtest(test_loader)\n",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, epochs, train_loader, save_path, patience)\u001b[0m\n\u001b[1;32m     51\u001b[0m train_f1 \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mF1Score(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     52\u001b[0m train_auroc \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mAUROC(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     56\u001b[0m     features \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniforge3/envs/thesis/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 127, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 127, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [254, 1, 1, 112, 112] at entry 0 and [254, 1, 112, 112] at entry 1\n"
     ]
    }
   ],
   "source": [
    "train(PATH, NUM_EPOCHS, KFOLD, BATCH_SIZE, NUM_WORKERS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualising cofusion matrix\n",
    "from torchmetrics import ConfusionMatrix\n",
    "import matplotlib\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "cmat = ConfusionMatrix(task='multiclass', num_classes=NUM_CLASSES)\n",
    "\n",
    "for x, y in dm.test_dataloader():\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = lightning_model(x)\n",
    "    cmat(pred, y)\n",
    "\n",
    "cmat_tensor = cmat.compute()\n",
    "cmat = cmat_tensor.numpy()\n",
    "\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=cmat,\n",
    "    class_names=class_dict.values(),\n",
    "    norm_colormap=matplotlib.colors.LogNorm()  \n",
    "    # normed colormaps highlight the off-diagonals \n",
    "    # for high-accuracy models better\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(path, size=(112,112)):\n",
    "    image = cv2.imread(path, 0)\n",
    "    if image is None:\n",
    "        return np.zeros()\n",
    "    \n",
    "    image = cv2.resize(image, size) / 255\n",
    "    return image.astype('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths =[]\n",
    "images=[load_image(path) for path in paths]\n",
    "org_size = len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dup tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3239/2694937284.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images = [torch.tensor(image, dtype=torch.float32) for image in images]\n"
     ]
    }
   ],
   "source": [
    "dup_len = 254 - len(images)\n",
    "if org_size == 0:\n",
    "    dup = torch.zeros(1, 112, 112)\n",
    "    print('dup', dup)\n",
    "else:\n",
    "    dup = images[-1]\n",
    "for i in range(dup_len):\n",
    "    images.append(dup)\n",
    "\n",
    "images = [torch.tensor(image, dtype=torch.float32) for image in images]\n",
    "images = torch.stack(images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([254, 1, 112, 112])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = load_image('data/reduced_dataset/00002/FLAIR/Image-437.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = plt.imread('data/reduced_dataset/00002/FLAIR/Image-437.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = []\n",
    "image.append(img)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup = image[-1]\n",
    "dup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    image.append(dup)\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = [torch.tensor(image, dtype=torch.float32) for image in image]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112, 112])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.stack(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 112, 112])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "img = load_image('data/reduced_dataset/00002/FLAIR/Image-437.png')\n",
    "images.append(img)\n",
    "\n",
    "dup = images[-1]\n",
    "\n",
    "for i in range(4):\n",
    "    images.append(dup)\n",
    "\n",
    "images = [torch.tensor(image, dtype=torch.float32) for image in images]\n",
    "images = torch.vstack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([254, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org = [0]\n",
    "masks= []\n",
    "for i in org:\n",
    "    #print(i)\n",
    "    dup = 254 - i\n",
    "    mask_1 = torch.ones(i, 64) # .to(device='cuda')\n",
    "    mask_0 = torch.zeros(dup, 64) #.to(device='cuda')\n",
    "    mask = torch.cat((mask_1, mask_0), 0)\n",
    "    masks.append(mask)\n",
    "\n",
    "masks = torch.stack(masks)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.zeros((1,112,112))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "org_size = 150\n",
    "dup_len = 254 - org_size\n",
    "dup = np.zeros((1,112,112))\n",
    "for i in range(150):\n",
    "    images.append(dup)\n",
    "\n",
    "dup = images[-1]\n",
    "for i in range(dup_len):\n",
    "    images.append(dup)\n",
    "\n",
    "images = [torch.tensor(image, dtype=torch.float32) for image in images]\n",
    "images = torch.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([254, 1, 112, 112])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "dup = np.zeros((1,112,112))  #torch.zeros(1, 112, 112).numpy()\n",
    "\n",
    "for i in range(254):\n",
    "    images.append(dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)\n",
    "\n",
    "#[tensor([0])]\n",
    "#feature shape torch.Size([1, 254, 1, 1, 112, 112])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([254, 1, 112, 112])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = [torch.tensor(image, dtype=torch.float32) for image in images]\n",
    "images = torch.stack(images)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 254, 1, 112, 112])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =[]\n",
    "data.append(images)\n",
    "data = torch.stack(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([254, 1, 1, 112, 112])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.transpose(1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 254, 1, 112, 112])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d =[]\n",
    "d.append(images)\n",
    "d = torch.stack(d)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([254, 1, 1, 112, 112])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.transpose(1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
