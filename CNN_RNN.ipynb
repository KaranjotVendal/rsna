{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'! pip install lightning\\n! pip install torchmetrics\\n! pip install watermark\\n! pip install mlxtend\\n! pip install tensorboard\\n! pip install pandas'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''! pip install lightning\n",
    "! pip install torchmetrics\n",
    "! pip install watermark\n",
    "! pip install mlxtend\n",
    "! pip install pandas'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger, CSVLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import torchmetrics\n",
    "\n",
    "import timm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datamodules import RSNAdataset\n",
    "from plotting import show_failures, plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaranjot\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/karanjotvendal/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='a2a7828ed68b3cba08f2703971162138c680b664')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Karanjot Vendal\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.4\n",
      "IPython version      : 8.14.0\n",
      "\n",
      "torch: 2.0.1\n",
      "\n",
      "lightning   : 2.0.6\n",
      "timm        : 0.9.2\n",
      "pandas      : 2.0.3\n",
      "numpy       : 1.25.1\n",
      "matplotlib  : 3.7.2\n",
      "torchmetrics: 1.0.1\n",
      "torchvision : 0.15.2\n",
      "torch       : 2.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a 'Karanjot Vendal' -v -p torch --iversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/reduced_dataset/'\n",
    "MODEL = 'resnet18'\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"RACNet\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"experiment_{1}\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": LEARNING_RATE,\n",
    "      \"architecture\": \"CNN-GRU-MASK-FC-Classifier\",\n",
    "      \"dataset\": \"MICAA MRI\",\n",
    "      \"epochs\": NUM_EPOCHS,\n",
    "      \"BATCH_SIZE\": BATCH_SIZE\n",
    "      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RACNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = timm.create_model(MODEL, pretrained=True, num_classes=0, in_chans=1)\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        in_features = self.cnn(torch.randn(2, 1, 112, 112)).shape[1]\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=in_features, hidden_size=64, batch_first= True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(16256, 32, bias=True)\n",
    "        self.classifier = nn.Linear(32, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x, org):\n",
    "        # x shape: BxSxCxHxW\n",
    "        batch_size, slices, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * slices, C, H, W)\n",
    "        #print('reshape input', c_in.shape)\n",
    "        \n",
    "        out = self.cnn(c_in)\n",
    "        #print('CNN ouput', out.shape)\n",
    "        \n",
    "        rnn_in = out.view(batch_size, slices, -1)\n",
    "        #print('reshaped rnn_in', rnn_in.shape)\n",
    "        out, hd = self.rnn(rnn_in)\n",
    "        #out =F.relu(self.RNN(out))\n",
    "\n",
    "        #print('RNN ouput', out.shape)\n",
    "        mask = self.mask_layer(org)\n",
    "        out = out * mask\n",
    "        #print('mask ouput', out.shape)\n",
    "        \n",
    "        batch, slices, rnn_features = out.size()\n",
    "        out = out.reshape(batch_size, slices * rnn_features)\n",
    "        #print('reshaped masked output', out.shape)\n",
    "        \n",
    "        out = F.relu(self.fc(out))\n",
    "        #print('fc ouput', out.shape)\n",
    "        \n",
    "        logits = self.classifier(out)\n",
    "        #print('classifier ouput', logits.shape)\n",
    "        #output = F.softmax(logits, dim=1)\n",
    "        #[prob 0, prob 1]\n",
    "\n",
    "        return logits       \n",
    "\n",
    "    def mask_layer(self, org):\n",
    "        masks = []\n",
    "        for i in org:\n",
    "            dup = 254 - i\n",
    "            mask_1 = torch.ones(i, 64).to(device='cuda')\n",
    "            mask_0 = torch.zeros(dup, 64).to(device='cuda')\n",
    "            mask = torch.cat((mask_1, mask_0), 0)\n",
    "            masks.append(mask)\n",
    "\n",
    "        masks = torch.stack(masks).to(device='cuda')\n",
    "        return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuring the lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Configuring the lightning module\n",
    "class LightningModel(L.LightningModule):\n",
    "    def __init__(self, model, learning_rate, num_epochs, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        #self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        \n",
    "        self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=2, average='macro')\n",
    "        #self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=2, average='macro')\n",
    "        self.test_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=2, average='macro')\n",
    "\n",
    "        self.train_auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=2)\n",
    "        #self.val_auroc = AUROC(task=\"multiclass\", num_classes=2)\n",
    "        self.test_auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=2)\n",
    "\n",
    "    def forward(self, x, org):\n",
    "        return self.model(x, org)\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        dict, org = batch\n",
    "        features = dict['X']\n",
    "        print('input',features.shape)\n",
    "        true_labels = dict['y']\n",
    "        true_labels = true_labels.type(torch.cuda.LongTensor)\n",
    "        print('targets',true_labels.shape)\n",
    "        print('org', org[0])\n",
    "        out = self(features, org[0])\n",
    "\n",
    "        loss = F.cross_entropy(out, true_labels)\n",
    "        predicted_labels = torch.softmax(out, dim=1)\n",
    "        return loss, true_labels, predicted_labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\n",
    "            \"train_acc\", self.train_acc, prog_bar=True, on_epoch=True, on_step=False\n",
    "        )\n",
    "        self.train_f1(predicted_labels, true_labels)\n",
    "        self.log(\"train_f1\", self.train_f1, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "        self.train_auroc(predicted_labels, true_labels)\n",
    "        self.log(\"train_auroc\", self.train_auroc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    '''def validation_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=False)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=False)'''\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc, on_epoch=True)\n",
    "        \n",
    "        sef.test_f1(predicted_labels, true_labels)\n",
    "        self.log(\"train_f1\", self.test_f1, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "        self.test_auroc(predicted_labels, true_labels)\n",
    "        self.log(\"train_auroc\", self.test_auroc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RSNADataModule(L.LightningDataModule):\n",
    "    def __init__(self, fold, data_path=PATH, batch_size=10, num_workers=0, height_width=(112,112), mod=\"FLAIR\"):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = data_path\n",
    "        self.height_width = height_width\n",
    "        self.num_workers = num_workers\n",
    "        self.fold = fold\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        self.train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.height_width),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5), (0.5))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.height_width),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5), (0.5))\n",
    "            ]\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Note transforms.ToTensor() scales input images\n",
    "        # to 0-1 range\n",
    "        folds_xtrain = np.load('./data/folds/xtrain.npy', allow_pickle=True)\n",
    "        folds_xtest = np.load('./data/folds/xtest.npy', allow_pickle=True)\n",
    "        folds_ytrain = np.load('./data/folds/ytrain.npy', allow_pickle=True)\n",
    "        folds_ytest = np.load('./data/folds/ytest.npy', allow_pickle=True)\n",
    "        \n",
    "        xtrain = folds_xtrain[self.fold]\n",
    "        ytrain = folds_ytrain[self.fold]\n",
    "        xtest = folds_xtest[self.fold]\n",
    "        ytest = folds_ytest[self.fold]\n",
    "\n",
    "        \n",
    "        self.train = RSNAdataset(\n",
    "            self.data_path,\n",
    "            xtrain,  \n",
    "            ytrain,\n",
    "            n_slices=254,\n",
    "            img_size=112,\n",
    "            transform=None\n",
    "        )\n",
    "\n",
    "        self.test = RSNAdataset(\n",
    "            self.data_path,\n",
    "            xtest,  \n",
    "            ytest,\n",
    "            n_slices=254,\n",
    "            img_size=112,\n",
    "            transform=None\n",
    "        )\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            dataset=self.train,\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=False,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    '''def val_dataloader(self):\n",
    "        valid_loader = DataLoader(\n",
    "            dataset=self.valid,\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        return valid_loader'''\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_loader = DataLoader(\n",
    "            dataset=self.test,\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = RSNADataModule(4, PATH, BATCH_SIZE, NUM_WORKERS)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting up trainer and logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wandb_logger = WandbLogger(\\n      project=\"RACNet\", \\n      name=f\"experiment_{2}\", \\n      # Track hyperparameters and run metadata\\n      config={\\n      \"learning_rate\": LEARNING_RATE,\\n      \"architecture\": \"CNN-LSTM\",\\n      \"dataset\": \"MICAA MRI\",\\n      \"epochs\": NUM_EPOCHS,\\n      \"batch size\": BATCH_SIZE,\\n      \"Image size\": (112,112)\\n      })'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#config lighnting module\n",
    "pymodel = RACNet(num_classes=NUM_CLASSES)\n",
    "lightning_model = LightningModel(pymodel, learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "#checkpointing the best model\n",
    "#configuring earlystopping\n",
    "callbacks = [ModelCheckpoint(dirpath ='checkpoints/', \n",
    "                             filename= 'RACNet_fold{4}_f1:{train_f1}', \n",
    "                             save_top_k=1, mode='max', monitor='train_f1'),\n",
    "            TQDMProgressBar(refresh_rate=50),\n",
    "            EarlyStopping(monitor=\"train_loss\", mode=\"min\", patience=5)\n",
    "            ]\n",
    "\n",
    "#configuring logger\n",
    "csv_logger = CSVLogger(save_dir='csv_logs/', name='ResNet18')\n",
    "'''wandb_logger = WandbLogger(\n",
    "      project=\"RACNet\", \n",
    "      name=f\"experiment_{2}\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": LEARNING_RATE,\n",
    "      \"architecture\": \"CNN-LSTM\",\n",
    "      \"dataset\": \"MICAA MRI\",\n",
    "      \"epochs\": NUM_EPOCHS,\n",
    "      \"batch size\": BATCH_SIZE,\n",
    "      \"Image size\": (112,112)\n",
    "      })'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    callbacks = callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=[csv_logger],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | model       | RACNet             | 11.8 M\n",
      "1 | train_acc   | MulticlassAccuracy | 0     \n",
      "2 | test_acc    | MulticlassAccuracy | 0     \n",
      "3 | train_f1    | MulticlassF1Score  | 0     \n",
      "4 | test_f1     | MulticlassF1Score  | 0     \n",
      "5 | train_auroc | MulticlassAUROC    | 0     \n",
      "6 | test_auroc  | MulticlassAUROC    | 0     \n",
      "---------------------------------------------------\n",
      "631 K     Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.8 M    Total params\n",
      "47.206    Total estimated model params size (MB)\n",
      "/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                  | 0/234 [00:00<?, ?it/s]input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([35, 95], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([137,  32], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([107, 203], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([16, 17], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:42: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:42: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([32, 34], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([ 32, 104], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([112, 201], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([33, 48], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([100,  33], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([107, 103], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([33, 97], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([106,  32], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([ 35, 112], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([33, 17], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([207,  33], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([100, 106], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([35, 34], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([100,  33], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([100,  34], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([107,  35], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([35, 90], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([100,  35], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([15, 34], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([ 17, 101], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([102, 100], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([50, 16], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([105,  34], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([31, 50], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([33, 11], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([ 16, 103], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([34, 18], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([112,  19], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([35, 34], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([ 31, 106], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([31, 34], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([ 16, 194], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([92, 96], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([20, 32], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([14, 96], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([15, 29], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([123,  11], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([107, 194], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([ 33, 104], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([99, 31], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([35, 34], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([34, 35], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([15, 99], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([ 35, 100], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([33, 33], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([17, 31], device='cuda:0')\n",
      "Epoch 0:  21%|█████████████▍                                                 | 50/234 [00:11<00:42,  4.34it/s, v_num=18]input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([107, 107], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([ 31, 105], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([34, 14], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([189, 110], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([32, 32], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([34, 35], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([100, 102], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([96, 99], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([31, 22], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([128,  31], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([100,  33], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([ 33, 109], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([104, 202], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([ 34, 100], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([106, 106], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([34, 32], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([47, 32], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([35, 34], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([102,  35], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([35, 39], device='cuda:0')\n",
      "input torch.Size([2, 254, 1, 112, 112])\n",
      "targets torch.Size([2])\n",
      "org tensor([15, 32], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karanjotvendal/karanjot/thesis/RSNA/4 Task custom architecture/datamodules.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images = [torch.tensor(image, dtype=torch.float32) for image in images]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "trainer.fit(model=lightning_model, datamodule=dm)\n",
    "runtime = (time.time() - start_time)/60\n",
    "print(f'Training finished in {runtime: .2f} min in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting loss and accuracy\n",
    "plot_loss_and_acc(trainer.logger.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating accuracy of Best(Checkpoint) model on test set\n",
    "trainer.test(model=lightning_model, datamodule=dm, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualising cofusion matrix\n",
    "from torchmetrics import ConfusionMatrix\n",
    "import matplotlib\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "cmat = ConfusionMatrix(task='multiclass', num_classes=NUM_CLASSES)\n",
    "\n",
    "for x, y in dm.test_dataloader():\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = lightning_model(x)\n",
    "    cmat(pred, y)\n",
    "\n",
    "cmat_tensor = cmat.compute()\n",
    "cmat = cmat_tensor.numpy()\n",
    "\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=cmat,\n",
    "    class_names=class_dict.values(),\n",
    "    norm_colormap=matplotlib.colors.LogNorm()  \n",
    "    # normed colormaps highlight the off-diagonals \n",
    "    # for high-accuracy models better\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
