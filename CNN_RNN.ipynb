{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'! pip install lightning\\n! pip install torchmetrics\\n! pip install watermark\\n! pip install mlxtend\\n! pip install tensorboard\\n! pip install pandas'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''! pip install lightning\n",
    "! pip install torchmetrics\n",
    "! pip install watermark\n",
    "! pip install mlxtend\n",
    "! pip install tensorboard\n",
    "! pip install pandas'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import torchmetrics\n",
    "\n",
    "import timm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datamodules import RSNAdataset\n",
    "from plotting import show_failures, plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key='a2a7828ed68b3cba08f2703971162138c680b664')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Karanjot Vendal\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.4\n",
      "IPython version      : 8.14.0\n",
      "\n",
      "torch: 2.0.1\n",
      "\n",
      "lightning   : 2.0.6\n",
      "timm        : 0.9.2\n",
      "pandas      : 2.0.3\n",
      "numpy       : 1.25.1\n",
      "matplotlib  : 3.7.2\n",
      "torchmetrics: 1.0.1\n",
      "torchvision : 0.15.2\n",
      "torch       : 2.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a 'Karanjot Vendal' -v -p torch --iversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "#DATASET = 'MNIST' #CIFAR or MNIST\n",
    "NUM_WORKERS = 6\n",
    "GRAYSCALE = True\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Kaggle LB-2 Baseline run\", \n",
    "      # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"experiment_{2}\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": LEARNING RATE,\n",
    "      \"architecture\": \"CNN-LSTM\",\n",
    "      \"dataset\": \"MICAA MRI\",\n",
    "      \"epochs\": NUM_EPOCHS,\n",
    "      \"BATCH_SIZE\": BATCH_SIZE\n",
    "      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecNet(nn.Module):\n",
    "    def __init_(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #self.CNN = timm.create_model('resnet50', pretrained=True, num_classes=0)\n",
    "        self.CNN = timm.create_model('resnet50', pretrained=True, num_classes=0, in_chans=1)\n",
    "        in_feature = self.CNN.fc.in_feature\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=in_feature, hidden_size=64, batch_first= True, bidirectional=False)\n",
    "        \n",
    "        self.fc = self.Linear(hidden_size, 32, bias=True)\n",
    "        self.classifier = self.Linear(32, num_calsses=2, bias=True)\n",
    "\n",
    "    def forward(self, x, mask_in, mask_dup):\n",
    "        mask = mask_layer(mask_in, mask_dup)\n",
    "        \n",
    "        out = self.CNN(x)\n",
    "        out = self.RNN(out)\n",
    "        out = out * mask\n",
    "        out = self.fc(out)\n",
    "\n",
    "        logits = self.classifier(out)\n",
    "        output = F.softmax(logits, dim=1)\n",
    "        #output = F.softmax(logits) #[prob 0, prob 1]\n",
    "\n",
    "    def mask_layer(self, mask_in, mask_dup):\n",
    "        mask_1 = torch.ones(mask_in, 64)\n",
    "        mask_0 = torch.zeros(mask_dup, 64)\n",
    "        \n",
    "        return torch.cat((mask_1, mask_0), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuring the lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuring the lightning module\n",
    "class LightningModel(L.LightningModule):\n",
    "    def __init__(self, model, learning_rate, num_epochs, batch_size, dataset, grayscale, height_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.height_width = height_width\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        \n",
    "        self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=2)\n",
    "        self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=2)\n",
    "        self.test_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=2)\n",
    "\n",
    "        self.train_auroc = AUROC(task=\"multiclass\", num_classes=2)\n",
    "        self.val_auroc = AUROC(task=\"multiclass\", num_classes=2)\n",
    "        self.test_auroc = AUROC(task=\"multiclass\", num_classes=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x, mask_in, mask_dup)\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        features, true_labels = batch\n",
    "        out = self(features)\n",
    "\n",
    "        loss = F.cross_entropy(out, true_labels)\n",
    "        predicted_labels = torch.argmax(out, dim=1)\n",
    "        return loss, true_labels, predicted_labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\n",
    "            \"train_acc\", self.train_acc, prog_bar=True, on_epoch=True, on_step=False\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=False)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=False)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MriDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_path=\"./data/reduced_dataset/\", batch_size=10, num_workers=0, height_width=(112,112), mod=\"FLAIR\"):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = data_path\n",
    "        self.height_width = height_width\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        self.train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.height_width),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5), (0.5))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.height_width),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5), (0.5))\n",
    "            ]\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Note transforms.ToTensor() scales input images\n",
    "        # to 0-1 range\n",
    "        folds_xtrain = np.load('./data/folds/xtrain.npy', allow_pickle=True)\n",
    "        folds_xtest = np.load('./data/folds/xtest.npy', allow_pickle=True)\n",
    "        folds_ytrain = np.load('./data/folds/ytrain.npy', allow_pickle=True)\n",
    "        folds_ytest = np.load('./data/folds/ytest.npy', allow_pickle=True)\n",
    "        \n",
    "        xtrain = folds_xtrain[0]\n",
    "        ytrain = folds_ytrain[0]\n",
    "        xtest = folds_xtest[0]\n",
    "        ytest = folds_ytest[0]\n",
    "\n",
    "        \n",
    "        self.train = RSNAdataset(\n",
    "            'data/reduced_dataset/',\n",
    "            xtrain,  \n",
    "            ytrain,\n",
    "            n_slices=254,\n",
    "            img_size=112,\n",
    "            transform=None\n",
    "        )\n",
    "\n",
    "        '''self.test = datasets(\n",
    "            root=self.data_path,\n",
    "            train=False,\n",
    "            transform=self.test_transform,\n",
    "            download=False,\n",
    "        )'''\n",
    "\n",
    "        #self.train, self.valid = random_split(train, lengths=[55000, 5000])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            dataset=self.train,\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    '''def val_dataloader(self):\n",
    "        valid_loader = DataLoader(\n",
    "            dataset=self.valid,\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        return valid_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_loader = DataLoader(\n",
    "            dataset=self.test,\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        return test_loader'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MriDataModule(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_counter = Counter()\n",
    "for images, labels in dm.train_dataloader():\n",
    "    train_counter.update(labels.tolist())\n",
    "\n",
    "#test_counter = Counter()\n",
    "#for images, labels in dm.test_dataloader():\n",
    "#    test_counter.update(labels.tolist())\n",
    "\n",
    "print(\"\\nTraining label distribution:\")\n",
    "sorted(train_counter.items())\n",
    "\n",
    "#print(\"\\nTest label distribution:\")\n",
    "#sorted(test_counter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in dm.train_dataloader():  \n",
    "    break\n",
    "'''\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training images\")\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(\n",
    "    images[:64], \n",
    "    padding=2,\n",
    "    normalize=True),\n",
    "    (1, 2, 0)))\n",
    "plt.show()'''\n",
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting up trainer and logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config lighnting module\n",
    "pymodel = resnet18(num_classes=NUM_CLASSES)\n",
    "lightning_model = LightningModel(pymodel, learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, dataset=DATASET, grayscale=GRAYSCALE,\n",
    "                                 height_width=height_width)\n",
    "\n",
    "#checkpointing the best model\n",
    "#configuring earlystopping\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode='max', monitor='val_acc'),\n",
    "            TQDMProgressBar(refresh_rate=50),\n",
    "            EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n",
    "            ]\n",
    "\n",
    "#configuring logger\n",
    "csv_logger = CSVLogger(save_dir='csv_logs/', name='ResNet18')\n",
    "tb_logger = TensorBoardLogger(save_dir='tb_logs/', name='ResNet18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    callbacks = callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=[csv_logger, tb_logger],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "trainer.fit(model=lightning_model, datamodule=dm)\n",
    "runtime = (time.time() - start_time)/60\n",
    "print(f'Training finished in {runtime: .2f} min in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
