{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import timm\n",
    "\n",
    "from utils import load_image, LossMeter, save_metrics_to_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle lb-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6294426941163301"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiments2.0/experiment 1/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02812416876460955"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiments2.0/experiment 1/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0247915313256325, 0.04891987898308514]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 1/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5954630615286354"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiments2.0/experiment 1/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04636449937213588"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiments2.0/experiment 1/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02813285075580163, 0.09216734085586531]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 1/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5549955791335102"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiments2.0/experiment 1/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02101349035675282"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiments2.0/experiment 1/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.029133510167992926, 0.031211317418213924]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 1/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle lb-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47446451187133787"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 2\\efficientb0_lstm\\metrics_efficientb0_lstm.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24168294294306444"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 2\\efficientb0_lstm\\metrics_efficientb0_lstm.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47446451187133787, 0.18343021869659426]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 2/efficientb0_lstm/metrics_efficientb0_lstm.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5730753540992737"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 2\\efficientb0_lstm\\metrics_efficientb0_lstm.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043676495828542904"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 2\\efficientb0_lstm\\metrics_efficientb0_lstm.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.049070656299591064, 0.07819139957427979]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 2/efficientb0_lstm/metrics_efficientb0_lstm.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5326260030269623"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 2\\efficientb0_lstm\\metrics_efficientb0_lstm.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03226900691280389"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 2\\efficientb0_lstm\\metrics_efficientb0_lstm.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05848807096481323, 0.027718812227249146]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 2/efficientb0_lstm/metrics_efficientb0_lstm.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline convxlstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5247991979122162"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 3\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0677176741787494"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 3\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13177591562271118, 0.04756918549537659]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 3/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.578080689907074"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 3\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04409596699059872"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 3\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08598468303680418, 0.03264960050582888]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 3/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.554995596408844"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 3\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019495892153825252"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 3\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02508103847503662, 0.02259063720703125]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 3/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data augmentation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5261698842048645"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 4\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04484377881952494"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 4\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08587931394577031, 0.033356559276580766]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 4/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5959612607955933"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 4\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038681445655668015"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 4\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06317441463470463, 0.04748136997222896]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 4/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5481579780578614"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 4\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012137530790536718"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 4\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.018243420124054, 0.012186837196350053]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 4/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data augmentation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5387519240379334"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 5\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015376220435655088"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 5\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02821978330612185, 0.014116322994232156]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 5/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5772109985351562"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 5\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03509214903625858"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 5\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06550142765045164, 0.031432831287384055]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 5/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5515767812728882"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 5\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02049335627293575"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 5\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03875623941421513, 0.02107281684875484]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 5/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img size=224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5356306910514832"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 6\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05579899982982066"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 6\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11074911355972294, 0.03448565006256099]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 6/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5762667894363404"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 6\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04095062879312868"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 6\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0663135886192322, 0.046976780891418435]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 6/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5670350909233093"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 6\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022833842063043442"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 6\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03712053298950191, 0.022708523273468062]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 6/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convxlstm t1w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5220528364181518"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 7\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.040615953636257335"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 7\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05780376195907588, 0.05297522544860844]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 7/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5664669036865234"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 7\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04003661106322061"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 7\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.056183791160583474, 0.05350329875946047]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 7/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5412467002868653"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 7\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025585179079173084"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 7\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.028426158428192183, 0.036339533329009965]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 7/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convxlstm t1wce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5175851166248322"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 8\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08043218755393025"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 8\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15313707590103154, 0.07576165795326228]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 8/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5892857074737549"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 8\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02708421794380781"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 8\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03310093879699705, 0.03455927371978762]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 8/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.554995596408844"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 8\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024857630284687245"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 8\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02508103847503662, 0.03983199596405029]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 8/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convxlstm t2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.530352246761322"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 9\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043329443658280585"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 9\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06082178354263301, 0.05188064575195317]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 9/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5871471047401429"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 9\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05794339184121283"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 9\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0619714736938477, 0.1013774991035461]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 9/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5431034624576568"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 9\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04470996556094373"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 9\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04737696647644041, 0.06896548867225649]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 9/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretrain convxlstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6019711494445801"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 10\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04230288606731902"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 10\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05549269914627075, 0.0700526237487793]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 10/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5950106501579284"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 10\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037794433038838664"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 10\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05109966993331905, 0.060130941867828414]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 10/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6083701729774476"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 10\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.040359433031525"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\experiments2.0\\experiment 10\\ConvxLSTM\\metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.std(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.061361598968505904, 0.0640435934066772]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 10/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5450714230537415"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 13/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07971173524856567, 0.04358106851577759]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 13/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.578639554977417"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 13/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05639128684997563, 0.03358097076416011]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 13/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5550987422466278"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 13/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08501327037811279, 0.048349529504776]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 13/ConvxLSTM/metrics_ConvxLSTM.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle LB-1 rerun with raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5466245866964894"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 14/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16567220574410846, 0.09440105432915169]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 14/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['f1'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6045081967213115"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 14/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04295827123695983, 0.04049925484351724]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 14/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['auroc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5652961980548188"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 14/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.048054818744473926, 0.04677276746242254]\n"
     ]
    }
   ],
   "source": [
    "with open('./experiment 2.0/experiment 14/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "#avg_performance\n",
    "max = np.max(test_f1) - avg_performance\n",
    "min = avg_performance - np.min(test_f1)\n",
    "spread = [min, max]\n",
    "print(spread) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./experiment 2.0/experiment 14/resnet10/metrics_resnet10.json', \"r\") as file:\n",
    "        metrics_dict = json.load(file)\n",
    "\n",
    "folds = [1,2,3,4,5]\n",
    "\n",
    "test_f1 = [metrics_dict[str(fold)]['test']['acc'][0] for fold in folds]\n",
    "avg_performance = np.mean(np.array(test_f1))\n",
    "avg_performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
