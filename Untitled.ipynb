{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f962ed0b-f9ef-4e3b-86f8-9ad3d3a4fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import torchmetrics\n",
    "\n",
    "import timm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#from datamodules import Cifar10DataModule, MnistDataModule\n",
    "from plotting import show_failures, plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec111f-5f76-49d5-9cbc-d1d53203724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecNet(nn.Module):\n",
    "    def __init_(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #self.CNN = timm.create_model('resnet50', pretrained=True, num_classes=0)\n",
    "        self.CNN = timm.create_model('resnet50', pretrained=True, num_classes=0, in_chans=1)\n",
    "        in_feature = self.CNN.fc.in_feature\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=in_feature, hidden_size=64, batch_first= True, bidirectional=False)\n",
    "        \n",
    "        self.fc = self.Linear(hidden_size, 32, bias=True)\n",
    "        self.classifier = self.Linear(32, num_calsses=2, bias=True)\n",
    "\n",
    "    def forward(self, x, mask_in, mask_dup):\n",
    "        mask = mask_layer(mask_in, mask_dup)\n",
    "        \n",
    "        out = self.CNN(x)\n",
    "        out = self.RNN(out)\n",
    "        out = out * mask\n",
    "        out = self.fc(out)\n",
    "\n",
    "        logits = self.classifier(out)\n",
    "        output = F.softmax(logits, dim=1)\n",
    "        #output = F.softmax(logits) #[prob 0, prob 1]\n",
    "\n",
    "    def mask_layer(self, mask_in, mask_dup):\n",
    "        mask_1 = torch.ones(mask_in, 64)\n",
    "        mask_0 = torch.zeros(mask_dup, 64)\n",
    "        \n",
    "        return torch.cat((mask_1, mask_0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef909f1e-8865-46d9-94ef-46f35e0ca4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d9bfd-6e91-4ec5-bd5d-b3d438e5051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f61897-9036-4f2b-9b34-411ef52c4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = timm.create_model('resnet50', pretrained=True, num_classes=0)\n",
    "#m = res(torch.randn(2, 3, 224, 224))\n",
    "o = res(torch.randn(2, 3, 224, 224))\n",
    "print(f'Pooled shape: {o.shape}')\n",
    "print(res.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d35ca7-42d2-42e4-a934-b2ae59d81914",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_1 = torch.ones(200, 64)\n",
    "mask_0 = torch.zeros(50, 64)\n",
    "\n",
    "mask = torch.cat((mask_1, mask_0), 0)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e3c8404-7d7a-46aa-91d3-b286b0b7cb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 112, 112])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.ones(254, 1, 112, 112)\n",
    "cpy = out[-1]\n",
    "cpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba35189-57fb-4398-a5d9-0c565fc264f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = os.listdir('data/reduced_dataset/00005/FLAIR')\n",
    "dup_len = 254 - len(list)\n",
    "dup_len\n",
    "\n",
    "for i in range(dup_len):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2289f3a4-0d7f-42df-8841-c75651b12bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = list(pathlib.Path('data/reduced_dataset/00002/FLAIR/').rglob(\"*.png\"))\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f67db01-e1ad-4cea-9e5d-a1bd40aea523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17786f79-2876-4c23-a4fb-deb2da69363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, size=(112,112)):\n",
    "    image = cv2.imread(path, 0)\n",
    "    if image is None:\n",
    "        return np.zeros()\n",
    "    \n",
    "    image = cv2.resize(image, size) / 255\n",
    "    return image.astype('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f92448-a6ac-4fbc-80b4-9f6cca68e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in pathlib.Path('data/reduced_dataset/00002/FLAIR/').rglob(\"*.png\"):\n",
    "    print(path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff75be6-c631-411b-980f-745db61570d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for path in os.listdir('data/reduced_dataset/00002/FLAIR/'):\n",
    "    p.append(os.path.join('data/reduced_dataset/00002/FLAIR/', path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bb4cd34-4239-4b8f-9862-9f15d78dd28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/reduced_dataset/00002/FLAIR/Image-460.png'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f8eef26-3512-446b-92fb-2a40bc6e2bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "207\n",
      "254\n"
     ]
    }
   ],
   "source": [
    "#images = [load_image(path) for i in p]\n",
    "images=[]\n",
    "for i in p:\n",
    "    #print(i)\n",
    "    images.append(load_image(i))\n",
    "print(len(images))\n",
    "\n",
    "list = os.listdir('data/reduced_dataset/00005/FLAIR')\n",
    "dup_len = 254 - len(images)\n",
    "dup_len\n",
    "print(dup_len)\n",
    "\n",
    "dup = images[-1]\n",
    "for i in range(dup_len):\n",
    "    images.append(dup)\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b43ceb-ee6e-4793-ab3b-f5c40ef81e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [torch.tensor(frame, dtype=torch.float32) for frame in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5abc94e3-6372-4562-9c0b-1f6c0a7adf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "073d78be-7900-41a5-89a0-bb7e04d6c7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([254, 112, 112])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e6946df-9e6a-46e6-b48a-2f099d5fea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85567c9-1b9f-4578-961f-db1784fa96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_paths = sorted(\n",
    "                glob.glob(os.path.join('data/reduced_dataset/00005/', 'FLAIR', \"*\")), \n",
    "                key=lambda x: int(x[:-4].split(\"-\")[-1]),\n",
    "            )\n",
    "t_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "970d1485-453f-4a1b-a84e-d46d79e28ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNAdataset(Dataset):\n",
    "    def __init__(self, patient_path, paths, targets, n_slices, img_size, transform=None):\n",
    "        #(self, './data/reduced_dataset/', t['xtrain'],t['ytrain'], 254, 112, transform)\n",
    "        self.patient_path = patient_path\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.n_slices = n_slices\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "          \n",
    "    def __len__(self):\n",
    "        print(len(self.paths))\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def padding(self, paths):\n",
    "        images=[load_image(path) for path in paths]\n",
    "\n",
    "        dup_len = 254 - len(images)\n",
    "        dup = images[-1]\n",
    "        for i in range(dup_len):\n",
    "            images.append(dup)\n",
    "\n",
    "        images = [torch.tensor(image, dtype=torch.float32) for image in images]\n",
    "\n",
    "        if len(images)==0:\n",
    "            images = torch.zeros(self.n_slices, 112, 112)\n",
    "        else:\n",
    "            images = torch.stack(images)\n",
    "\n",
    "        return images\n",
    "    \n",
    "    '''def read_video(self, vid_paths):\n",
    "        video = [load_image(path, (self.img_size, self.img_size)) for path in vid_paths]\n",
    "        if self.transform:\n",
    "            seed = random.randint(0,99999)\n",
    "            for i in range(len(video)):\n",
    "                random.seed(seed)\n",
    "                video[i] = self.transform(image=video[i])[\"image\"]\n",
    "        \n",
    "        video = [torch.tensor(frame, dtype=torch.float32) for frame in video]\n",
    "        if len(video)==0:\n",
    "            video = torch.zeros(self.n_frames, self.img_size, self.img_size)\n",
    "        else:\n",
    "            video = torch.stack(video) # T * C * H * W\n",
    "        return video'''\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        _id = self.paths[index]\n",
    "        patient_path = os.path.join(self.patient_path, f'{str(_id).zfill(5)}/')\n",
    "\n",
    "        data = []\n",
    "        for t in [\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\"]:\n",
    "            t_paths = sorted(\n",
    "                glob.glob(os.path.join(patient_path, t, \"*\")), \n",
    "                key=lambda x: int(x[:-4].split(\"-\")[-1]),\n",
    "            )\n",
    "            num_samples = self.n_slices\n",
    "            ##if len(t_paths) < num_samples:\n",
    "             #   in_frames_path = t_paths\n",
    "            #else:\n",
    "             #   in_frames_path = uniform_temporal_subsample(t_paths, num_samples)\n",
    "            \n",
    "            image = self.padding(t_paths)\n",
    "            if image.shape[0] == 0:\n",
    "                image = torch.zeros(num_samples, self.img_size, self.img_size)\n",
    "            data.append(image)\n",
    "            break\n",
    "            \n",
    "        data = torch.stack(data).transpose(0,1)\n",
    "        #print(data.shape)\n",
    "        y = torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        return {\"X\": data.float(), \"y\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da24c94-3a29-4f66-93b3-091cf9f2f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Fold 1\n"
     ]
    }
   ],
   "source": [
    "folds_xtrain = np.load('./data/folds/xtrain.npy', allow_pickle=True)\n",
    "folds_xtest = np.load('./data/folds/xtest.npy', allow_pickle=True)\n",
    "folds_ytrain = np.load('./data/folds/ytrain.npy', allow_pickle=True)\n",
    "folds_ytest = np.load('./data/folds/ytest.npy', allow_pickle=True)\n",
    "\n",
    "xtrain = folds_xtrain[0]\n",
    "ytrain = folds_ytrain[0]\n",
    "xtest = folds_xtest[0]\n",
    "ytest = folds_ytest[0]\n",
    "\n",
    "print('-'*30)\n",
    "print(f\"Fold {'1'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a3c05b-4922-482d-aec8-152d40c21a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_retriever = RSNAdataset(\n",
    "    'data/reduced_dataset/',\n",
    "    xtrain,  \n",
    "    ytrain,\n",
    "    n_slices=254,\n",
    "    img_size=112,\n",
    "    transform=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14c75dc2-7d9d-45b9-a280-641dc92d0478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([254, 4, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "dict = train_retriever[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "565a6a35-88b0-4cd6-bda1-da5faa8ded8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "torch.Size([254, 1, 112, 112])\n",
      "169\n"
     ]
    }
   ],
   "source": [
    "for id, value in enumerate(train_retriever):\n",
    "    a =0\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7ae0cfc-d91e-439a-a3b4-f83038cb4de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7134597-ae95-459e-9a83-d57f1588631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5cfeee1b-da80-49df-84a9-845f11dfb38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112, 112])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "png = load_image('data/reduced_dataset/00002/FLAIR/Image-428.png')\n",
    "png = torch.tensor(png, dtype=torch.float32)\n",
    "png.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb0d2c47-aa04-48d4-b3b0-2a66b6ccce3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c0666-934a-4461-9e95-a8ea47e03f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a 'Karanjot Vendal' -v -p torch --iversion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
