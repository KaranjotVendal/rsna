{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f962ed0b-f9ef-4e3b-86f8-9ad3d3a4fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karanjotvendal/miniforge3/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import torchmetrics\n",
    "\n",
    "import timm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "#from datamodules import Cifar10DataModule, MnistDataModule\n",
    "from plotting import show_failures, plot_loss_and_acc\n",
    "from utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cec111f-5f76-49d5-9cbc-d1d53203724a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RecNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        ###self.CNN = timm.create_model('resnet50', pretrained=True, num_classes=0)\n",
    "        self.cnn = timm.create_model('resnet18', pretrained=True, num_classes=0, in_chans=1)\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        in_features = self.cnn(torch.randn(2, 1, 112, 112)).shape[1]\n",
    "        #in_feature = self.cnn.fc.in_features\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=in_features, hidden_size=64, batch_first= True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(16256, 32, bias=True)\n",
    "        self.classifier = nn.Linear(32, 2, bias=True)\n",
    "\n",
    "    def forward(self, x, org):\n",
    "        # x shape: BxTxCxHxW\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        print('reshape input', c_in.shape)\n",
    "        \n",
    "        mask = self.mask_layer(org)\n",
    "        \n",
    "        out = self.cnn(c_in)\n",
    "        print('CNN ouput', out.shape)\n",
    "        \n",
    "        rnn_in = out.view(batch_size, timesteps, -1)\n",
    "        print('reshaped rnn_in', rnn_in.shape)\n",
    "        out, hd = self.rnn(rnn_in)\n",
    "        \n",
    "        #out =F.relu(self.RNN(out))\n",
    "        print('RNN ouput', out.shape)\n",
    "        #print('RNN hidden', hd.shape)\n",
    "        \n",
    "        out = out * mask\n",
    "        print('mask ouput', out.shape)\n",
    "        \n",
    "        batch, timesteps, r_features = out.size() \n",
    "        #out = out.view(batch_size, timesteps * r_features)\n",
    "        out = out.reshape(batch_size, timesteps * r_features)\n",
    "        print('reshaped masked output', out.shape)\n",
    "        \n",
    "        out = F.relu(self.fc(out))\n",
    "        print('fc ouput', out.shape)\n",
    "\n",
    "        logits = self.classifier(out)\n",
    "        print('classifier ouput', logits.shape)\n",
    "        \n",
    "        #output = F.softmax(logits, dim=1)\n",
    "        #print('prb ouput', output.shape)\n",
    "        #output = F.softmax(logits) #[prob 0, prob 1]\n",
    "\n",
    "        #return output\n",
    "        return logits\n",
    "\n",
    "    def mask_layer(self, org):\n",
    "        masks = []\n",
    "        for i in org:\n",
    "            dup = 254 - i\n",
    "            mask_1 = torch.ones(i, 64)\n",
    "            mask_0 = torch.zeros(dup, 64)\n",
    "            mask = torch.cat((mask_1, mask_0), 0)\n",
    "            masks.append(mask)\n",
    "            #print(mask.shape)\n",
    "        masks = torch.stack(masks).to(device='cuda')\n",
    "        print('masks', masks.shape)\n",
    "        return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970d1485-453f-4a1b-a84e-d46d79e28ad6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RSNAdataset(Dataset):\n",
    "    def __init__(self, patient_path, paths, targets, n_slices, img_size, transform=None):\n",
    "        #(self, './data/reduced_dataset/', t['xtrain'],t['ytrain'], 254, 112, transform)\n",
    "        self.patient_path = patient_path\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.n_slices = n_slices\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "          \n",
    "    def __len__(self):\n",
    "        #print(len(self.paths))\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def padding(self, paths):\n",
    "        \n",
    "        images=[load_image(path) for path in paths]\n",
    "        org_size = len(images)\n",
    "\n",
    "        #if len(images) != 0:\n",
    "            \n",
    "        dup_len = 254 - len(images)\n",
    "        if org_size == 0:\n",
    "            dup = torch.zeros(self.n_slices, 112, 112)\n",
    "        else:\n",
    "            dup = images[-1]\n",
    "        for i in range(dup_len):\n",
    "            images.append(dup)\n",
    "\n",
    "        images = [torch.tensor(image, dtype=torch.float32) for image in images]\n",
    "\n",
    "        #if len(images)==0:\n",
    "        #    images = torch.zeros(self.n_slices, 112, 112)\n",
    "        #else:\n",
    "        images = torch.stack(images)\n",
    "\n",
    "        return images, org_size\n",
    "    \n",
    "    '''def read_video(self, vid_paths):\n",
    "        video = [load_image(path, (self.img_size, self.img_size)) for path in vid_paths]\n",
    "        if self.transform:\n",
    "            seed = random.randint(0,99999)\n",
    "            for i in range(len(video)):\n",
    "                random.seed(seed)\n",
    "                video[i] = self.transform(image=video[i])[\"image\"]\n",
    "        \n",
    "        video = [torch.tensor(frame, dtype=torch.float32) for frame in video]\n",
    "        if len(video)==0:\n",
    "            video = torch.zeros(self.n_frames, self.img_size, self.img_size)\n",
    "        else:\n",
    "            video = torch.stack(video) # T * C * H * W\n",
    "        return video'''\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        _id = self.paths[index]\n",
    "        patient_path = os.path.join(self.patient_path, f'{str(_id).zfill(5)}/')\n",
    "\n",
    "        data = []\n",
    "        org = []\n",
    "        for t in [\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\"]:\n",
    "            t_paths = sorted(\n",
    "                glob.glob(os.path.join(patient_path, t, \"*\")), \n",
    "                key=lambda x: int(x[:-4].split(\"-\")[-1]),\n",
    "            )\n",
    "            num_samples = self.n_slices\n",
    "            ##if len(t_paths) < num_samples:\n",
    "             #   in_frames_path = t_paths\n",
    "            #else:\n",
    "             #   in_frames_path = uniform_temporal_subsample(t_paths, num_samples)\n",
    "            \n",
    "            image, org_size = self.padding(t_paths)\n",
    "            if image.shape[0] == 0:\n",
    "                image = torch.zeros(num_samples, self.img_size, self.img_size)\n",
    "            data.append(image)\n",
    "            org.append(org_size)\n",
    "            break\n",
    "            \n",
    "        data = torch.stack(data).transpose(0,1)\n",
    "        #print(data.shape)\n",
    "        #print('after transpose', data.shape)\n",
    "        y = torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        return {\"X\": data.float(), \"y\": y}, org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da24c94-3a29-4f66-93b3-091cf9f2f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Fold 3\n"
     ]
    }
   ],
   "source": [
    "folds_xtrain = np.load('./data/folds/xtrain.npy', allow_pickle=True)\n",
    "folds_xtest = np.load('./data/folds/xtest.npy', allow_pickle=True)\n",
    "folds_ytrain = np.load('./data/folds/ytrain.npy', allow_pickle=True)\n",
    "folds_ytest = np.load('./data/folds/ytest.npy', allow_pickle=True)\n",
    "\n",
    "xtrain = folds_xtrain[4]\n",
    "ytrain = folds_ytrain[4]\n",
    "xtest = folds_xtest[4]\n",
    "ytest = folds_ytest[4]\n",
    "\n",
    "print('-'*30)\n",
    "print(f\"Fold {'3'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a3c05b-4922-482d-aec8-152d40c21a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_retriever = RSNAdataset(\n",
    "    'data/reduced_dataset/',\n",
    "    xtrain,  \n",
    "    ytrain,\n",
    "    n_slices=254,\n",
    "    img_size=112,\n",
    "    transform=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b2d6a9-46bf-4d18-ac9b-a763b699f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict, org = train_retriever[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07ff37b-448a-4998-bdf5-0b1612bc38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dict['X']\n",
    "targets = dict['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cacb7f55-52bc-4c85-a5f4-9c54157f6d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([254, 1, 112, 112])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape\n",
    "#targets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1957793-d329-426b-93c5-336a290688f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adbc7194-6d55-43e0-be38-c608ebab3e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd668472-faeb-430a-9204-5b4d22f63b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "            train_retriever,    \n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9279c8-5516-4908-86c3-db9f7caea158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecNet(\n",
       "  (cnn): ResNet(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (rnn): GRU(512, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=16256, out_features=32, bias=True)\n",
       "  (classifier): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RecNet()\n",
    "model.to(device='cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = F.cross_entropy\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22376b-155f-4e62-a416-cf7fedc97274",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = batch.to(device='cuda')\n",
    "X = X.unsqueeze(0)\n",
    "print(X.shape)\n",
    "y = targets.to(device='cuda')\n",
    "print(y.shape)\n",
    "\n",
    "#self.optimizer.zero_grad()\n",
    "outputs = model(X, org).squeeze()\n",
    "#outputs = outputs.squeeze()\n",
    "print(outputs.shape)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f1650b-8b1b-4830-a17a-302439aa40da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader output torch.Size([1, 254, 1, 112, 112])\n",
      "train_loader targets torch.Size([1])\n",
      "train org tensor([110])\n",
      "reshape input torch.Size([254, 1, 112, 112])\n",
      "masks torch.Size([1, 254, 64])\n",
      "CNN ouput torch.Size([254, 512])\n",
      "reshaped rnn_in torch.Size([1, 254, 512])\n",
      "RNN ouput torch.Size([1, 254, 64])\n",
      "mask ouput torch.Size([1, 254, 64])\n",
      "reshaped masked output torch.Size([1, 16256])\n",
      "fc ouput torch.Size([1, 32])\n",
      "classifier ouput torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader, 1):\n",
    "    X = batch[0]['X'].to(device='cuda')\n",
    "    print('train_loader output',X.shape)\n",
    "    y = batch[0]['y'].to(device='cuda')\n",
    "    print('train_loader targets',y.shape)\n",
    "    org = batch[1][0]\n",
    "    print('train org', org)\n",
    "    #count += 1\n",
    "    \n",
    "    #optimizer.zero_grad()\n",
    "    outputs = model(X, org).squeeze(1)\n",
    "    break\n",
    "    #print('prob outputs', outputs.shape)\n",
    "    \n",
    "    #loss = criterion(outputs, y)\n",
    "    #loss.backward()\n",
    "\n",
    "    #train_loss.update(loss.detach().item())\n",
    "    #train_score.update(targets, outputs.detach())\n",
    "    \n",
    "    #self.optimizer.step()\n",
    "    \n",
    "    #_loss, _score = train_loss.avg, train_score.avg\n",
    "    #message = 'Train Step {}/{}, train_loss: {:.5f}, train_score: {:.5f}, train_f1: {:.5f}'\n",
    "    #self.info_message(message, step, len(train_loader), _loss, _score, ff, end=\"\\r\")\n",
    "\n",
    "    #f_score = ff_score.get_score()\n",
    "    #return train_loss.avg, train_score.avg, f_score, int(time.time() - t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef909f1e-8865-46d9-94ef-46f35e0ca4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convnext_atto.d2_in1k',\n",
       " 'convnext_atto_ols.a2_in1k',\n",
       " 'convnext_base.clip_laion2b',\n",
       " 'convnext_base.clip_laion2b_augreg',\n",
       " 'convnext_base.clip_laion2b_augreg_ft_in1k',\n",
       " 'convnext_base.clip_laion2b_augreg_ft_in12k',\n",
       " 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k',\n",
       " 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384',\n",
       " 'convnext_base.clip_laiona',\n",
       " 'convnext_base.clip_laiona_320',\n",
       " 'convnext_base.clip_laiona_augreg_320',\n",
       " 'convnext_base.clip_laiona_augreg_ft_in1k_384',\n",
       " 'convnext_base.fb_in1k',\n",
       " 'convnext_base.fb_in22k',\n",
       " 'convnext_base.fb_in22k_ft_in1k',\n",
       " 'convnext_base.fb_in22k_ft_in1k_384',\n",
       " 'convnext_femto.d1_in1k',\n",
       " 'convnext_femto_ols.d1_in1k',\n",
       " 'convnext_large.fb_in1k',\n",
       " 'convnext_large.fb_in22k',\n",
       " 'convnext_large.fb_in22k_ft_in1k',\n",
       " 'convnext_large.fb_in22k_ft_in1k_384',\n",
       " 'convnext_large_mlp.clip_laion2b_augreg',\n",
       " 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k',\n",
       " 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384',\n",
       " 'convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384',\n",
       " 'convnext_large_mlp.clip_laion2b_ft_320',\n",
       " 'convnext_large_mlp.clip_laion2b_ft_soup_320',\n",
       " 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_320',\n",
       " 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_384',\n",
       " 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320',\n",
       " 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384',\n",
       " 'convnext_nano.d1h_in1k',\n",
       " 'convnext_nano.in12k',\n",
       " 'convnext_nano.in12k_ft_in1k',\n",
       " 'convnext_nano_ols.d1h_in1k',\n",
       " 'convnext_pico.d1_in1k',\n",
       " 'convnext_pico_ols.d1_in1k',\n",
       " 'convnext_small.fb_in1k',\n",
       " 'convnext_small.fb_in22k',\n",
       " 'convnext_small.fb_in22k_ft_in1k',\n",
       " 'convnext_small.fb_in22k_ft_in1k_384',\n",
       " 'convnext_small.in12k',\n",
       " 'convnext_small.in12k_ft_in1k',\n",
       " 'convnext_small.in12k_ft_in1k_384',\n",
       " 'convnext_tiny.fb_in1k',\n",
       " 'convnext_tiny.fb_in22k',\n",
       " 'convnext_tiny.fb_in22k_ft_in1k',\n",
       " 'convnext_tiny.fb_in22k_ft_in1k_384',\n",
       " 'convnext_tiny.in12k',\n",
       " 'convnext_tiny.in12k_ft_in1k',\n",
       " 'convnext_tiny.in12k_ft_in1k_384',\n",
       " 'convnext_tiny_hnf.a2h_in1k',\n",
       " 'convnext_xlarge.fb_in22k',\n",
       " 'convnext_xlarge.fb_in22k_ft_in1k',\n",
       " 'convnext_xlarge.fb_in22k_ft_in1k_384',\n",
       " 'convnext_xxlarge.clip_laion2b_rewind',\n",
       " 'convnext_xxlarge.clip_laion2b_soup',\n",
       " 'convnext_xxlarge.clip_laion2b_soup_ft_in1k',\n",
       " 'convnextv2_atto.fcmae',\n",
       " 'convnextv2_atto.fcmae_ft_in1k',\n",
       " 'convnextv2_base.fcmae',\n",
       " 'convnextv2_base.fcmae_ft_in1k',\n",
       " 'convnextv2_base.fcmae_ft_in22k_in1k',\n",
       " 'convnextv2_base.fcmae_ft_in22k_in1k_384',\n",
       " 'convnextv2_femto.fcmae',\n",
       " 'convnextv2_femto.fcmae_ft_in1k',\n",
       " 'convnextv2_huge.fcmae',\n",
       " 'convnextv2_huge.fcmae_ft_in1k',\n",
       " 'convnextv2_huge.fcmae_ft_in22k_in1k_384',\n",
       " 'convnextv2_huge.fcmae_ft_in22k_in1k_512',\n",
       " 'convnextv2_large.fcmae',\n",
       " 'convnextv2_large.fcmae_ft_in1k',\n",
       " 'convnextv2_large.fcmae_ft_in22k_in1k',\n",
       " 'convnextv2_large.fcmae_ft_in22k_in1k_384',\n",
       " 'convnextv2_nano.fcmae',\n",
       " 'convnextv2_nano.fcmae_ft_in1k',\n",
       " 'convnextv2_nano.fcmae_ft_in22k_in1k',\n",
       " 'convnextv2_nano.fcmae_ft_in22k_in1k_384',\n",
       " 'convnextv2_pico.fcmae',\n",
       " 'convnextv2_pico.fcmae_ft_in1k',\n",
       " 'convnextv2_tiny.fcmae',\n",
       " 'convnextv2_tiny.fcmae_ft_in1k',\n",
       " 'convnextv2_tiny.fcmae_ft_in22k_in1k',\n",
       " 'convnextv2_tiny.fcmae_ft_in22k_in1k_384']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('*convnext*', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d9bfd-6e91-4ec5-bd5d-b3d438e5051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4f61897-9036-4f2b-9b34-411ef52c4273",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled shape: 2048\n",
      "Identity()\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "res = timm.create_model('resnet50', pretrained=True, num_classes=0, in_chans=1)\n",
    "#m = res(torch.randn(2, 3, 224, 224))\n",
    "res.reset_classifier(0)\n",
    "o = res(torch.randn(2, 1, 112, 112))\n",
    "print(f'Pooled shape: {o.shape[1]}')\n",
    "print(res.fc)\n",
    "in_features = res(torch.randn(2, 1, 112, 112)).shape[1]\n",
    "print(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dddaa7e-49b7-4917-a0c1-d9f5052ac96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d35ca7-42d2-42e4-a934-b2ae59d81914",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_1 = torch.ones(200, 64)\n",
    "mask_0 = torch.zeros(50, 64)\n",
    "\n",
    "mask = torch.cat((mask_1, mask_0), 0)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e3c8404-7d7a-46aa-91d3-b286b0b7cb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 112, 112])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.ones(254, 1, 112, 112)\n",
    "cpy = out[-1]\n",
    "cpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba35189-57fb-4398-a5d9-0c565fc264f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = os.listdir('data/reduced_dataset/00123/FLAIR')\n",
    "dup_len = 254 - len(list)\n",
    "dup_len\n",
    "\n",
    "for i in range(dup_len):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2289f3a4-0d7f-42df-8841-c75651b12bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = list(pathlib.Path('data/reduced_dataset/00002/FLAIR/').rglob(\"*.png\"))\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f67db01-e1ad-4cea-9e5d-a1bd40aea523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f92448-a6ac-4fbc-80b4-9f6cca68e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in pathlib.Path('data/reduced_dataset/00002/FLAIR/').rglob(\"*.png\"):\n",
    "    print(path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff75be6-c631-411b-980f-745db61570d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for path in os.listdir('data/reduced_dataset/00002/FLAIR/'):\n",
    "    p.append(os.path.join('data/reduced_dataset/00002/FLAIR/', path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bb4cd34-4239-4b8f-9862-9f15d78dd28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/reduced_dataset/00002/FLAIR/Image-460.png'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f8eef26-3512-446b-92fb-2a40bc6e2bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "207\n",
      "254\n"
     ]
    }
   ],
   "source": [
    "#images = [load_image(path) for i in p]\n",
    "images=[]\n",
    "for i in p:\n",
    "    #print(i)\n",
    "    images.append(load_image(i))\n",
    "print(len(images))\n",
    "\n",
    "list = os.listdir('data/reduced_dataset/00005/FLAIR')\n",
    "dup_len = 254 - len(images)\n",
    "dup_len\n",
    "print(dup_len)\n",
    "\n",
    "dup = images[-1]\n",
    "for i in range(dup_len):\n",
    "    images.append(dup)\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b43ceb-ee6e-4793-ab3b-f5c40ef81e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [torch.tensor(frame, dtype=torch.float32) for frame in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5abc94e3-6372-4562-9c0b-1f6c0a7adf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "073d78be-7900-41a5-89a0-bb7e04d6c7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([254, 112, 112])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85567c9-1b9f-4578-961f-db1784fa96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_paths = sorted(\n",
    "                glob.glob(os.path.join('data/reduced_dataset/00005/', 'FLAIR', \"*\")), \n",
    "                key=lambda x: int(x[:-4].split(\"-\")[-1]),\n",
    "            )\n",
    "t_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38a4daa8-e539-491a-a104-f13847ecc5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "for i in xtrain:\n",
    "    if i == 123:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b164f18f-fb41-4d2b-a5fc-fe7d43968021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "407416f4-aa94-446d-a7c0-265d5b357621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([254, 64])\n",
      "torch.Size([254, 64])\n",
      "torch.Size([254, 64])\n",
      "torch.Size([254, 64])\n",
      "torch.Size([4, 254, 64])\n"
     ]
    }
   ],
   "source": [
    "masks = []\n",
    "org = [200, 180,170,210]\n",
    "\n",
    "for i in org:\n",
    "    dup = 254 - i\n",
    "    mask_1 = torch.ones(i, 64)\n",
    "    mask_0 = torch.zeros(dup, 64)\n",
    "    mask = torch.cat((mask_1, mask_0), 0)\n",
    "    masks.append(mask)\n",
    "    print(mask.shape)\n",
    "m = torch.stack(masks)\n",
    "print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b075869f-ff59-4cdf-a33c-5c9e1fc4f79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "becff45f-cada-492d-bdb6-173fc4368250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0bd04bf-be0b-4e9d-8f3e-e08053e7bb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80a9f025-3a97-4e6f-8787-1f22bde09676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb631612-56f5-4ad7-b62f-c63d02252005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c0666-934a-4461-9e95-a8ea47e03f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a 'Karanjot Vendal' -v -p torch --iversion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
